{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0cd030fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.8/dist-packages (3.2.1)\r\n",
      "Requirement already satisfied: py4j==0.10.9.3 in /usr/local/lib/python3.8/dist-packages (from pyspark) (0.10.9.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e43b644",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39cf5ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56d240b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Age</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ABC</td>\n",
       "      <td>21</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DEF</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GHI</td>\n",
       "      <td>27</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JKL</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name  Age  Experience\n",
       "0  ABC   21           2\n",
       "1  DEF   24           4\n",
       "2  GHI   27           6\n",
       "3  JKL   22           3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58738293",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27f785e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/31 11:05:02 WARN Utils: Your hostname, BTCCHL0018 resolves to a loopback address: 127.0.1.1; using 192.168.0.233 instead (on interface wlp44s0)\n",
      "22/03/31 11:05:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/stephenraj/spark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/31 11:05:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Practise').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48d6e84b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.233:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f874eeb8bb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bc0919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4074708f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "| _c0|_c1|       _c2|\n",
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "| ABC| 21|         2|\n",
      "| DEF| 24|         4|\n",
      "| GHI| 27|         6|\n",
      "| JKL| 22|         3|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6fba37e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1608c56f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+\n",
      "|Name|Age|Experience|\n",
      "+----+---+----------+\n",
      "| ABC| 21|         2|\n",
      "| DEF| 24|         4|\n",
      "| GHI| 27|         6|\n",
      "| JKL| 22|         3|\n",
      "+----+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('dataset.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7582a9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header','true').csv('dataset.csv').show()\n",
    "df_pyspark = spark.read.option('header','true').csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "12408888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0ff11ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Name='ABC', Age='21', Experience='2', Salary='30000')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d537b917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='ABC', Age='21', Experience='2', Salary='30000'),\n",
       " Row(Name='DEF', Age=None, Experience='4', Salary='20000'),\n",
       " Row(Name=None, Age='27', Experience='6', Salary='25000'),\n",
       " Row(Name='JKL', Age='22', Experience=None, Salary='40000')]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48a4c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5ca8f",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a850d0c7",
   "metadata": {},
   "source": [
    "Updated CSV File and proceeding for dataFrame manipulations - Steps from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b454f15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "04a370a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "09ab54a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.0.233:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Practise</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f874eeb8bb0>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7d811252",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: string, Experience: string, Salary: string]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Dataset\n",
    "spark.read.option('header', 'true').csv('dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19d83518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.read.option('header', 'true').csv('dataset.csv').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a09b7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header', 'true').csv('dataset.csv',inferSchema=True) #inferSchema - Used to detect datatypes of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "77cc861d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check Schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b835fa36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('dataset.csv', header = True, inferSchema = True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5c63822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "1b24702e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d51471ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4b916707",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='ABC', Age=21, Experience=2, Salary=30000),\n",
       " Row(Name='DEF', Age=None, Experience=4, Salary=20000),\n",
       " Row(Name=None, Age=27, Experience=6, Salary=25000),\n",
       " Row(Name='JKL', Age=22, Experience=None, Salary=40000)]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "acc95caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "15bac27c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3db952f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "| ABC|\n",
      "| DEF|\n",
      "|null|\n",
      "| JKL|\n",
      "| MNO|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1de0efb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark.select('Name'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "407a17e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "|Name|Experience|\n",
      "+----+----------+\n",
      "| ABC|         2|\n",
      "| DEF|         4|\n",
      "|null|         6|\n",
      "| JKL|      null|\n",
      "| MNO|         8|\n",
      "+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d8ed03a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Name'>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark['Name'] #Will not return the values (Only select has to be used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "25ffd55b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int'), ('Salary', 'int')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9b3d7d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string, Name: string, Age: string, Experience: string, Salary: string]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ce51f6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+-----------------+-----------------+-----------------+\n",
      "|summary|Name|              Age|       Experience|           Salary|\n",
      "+-------+----+-----------------+-----------------+-----------------+\n",
      "|  count|   4|                4|                4|                4|\n",
      "|   mean|null|             25.0|              5.0|          28750.0|\n",
      "| stddev|null|4.242640687119286|2.581988897471611|8539.125638299665|\n",
      "|    min| ABC|               21|                2|            20000|\n",
      "|    max| MNO|               30|                8|            40000|\n",
      "+-------+----+-----------------+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2e20c20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: int, Experience: int, Salary: int, Experience After 2 years: int]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding Columns\n",
    "df_pyspark.withColumn('Experience After 2 years', df_pyspark['Experience']+2) #withColumn - Used to manipulate with the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b19f43f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+------------------------+\n",
      "|Name| Age|Experience|Salary|Experience After 2 years|\n",
      "+----+----+----------+------+------------------------+\n",
      "| ABC|  21|         2| 30000|                       4|\n",
      "| DEF|null|         4| 20000|                       6|\n",
      "|null|  27|         6| 25000|                       8|\n",
      "| JKL|  22|      null| 40000|                    null|\n",
      "| MNO|  30|         8|  null|                      10|\n",
      "+----+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.withColumn('Experience After 2 years', df_pyspark['Experience']+2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "45deb20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Experience After 2 years', df_pyspark['Experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bb52c04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+------------------------+\n",
      "|Name| Age|Experience|Salary|Experience After 2 years|\n",
      "+----+----+----------+------+------------------------+\n",
      "| ABC|  21|         2| 30000|                       4|\n",
      "| DEF|null|         4| 20000|                       6|\n",
      "|null|  27|         6| 25000|                       8|\n",
      "| JKL|  22|      null| 40000|                    null|\n",
      "| MNO|  30|         8|  null|                      10|\n",
      "+----+----+----------+------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "06ae92b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.drop('Experience After 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "377fa1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "90454abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----+----------+------+\n",
      "|New Name| Age|Experience|Salary|\n",
      "+--------+----+----------+------+\n",
      "|     ABC|  21|         2| 30000|\n",
      "|     DEF|null|         4| 20000|\n",
      "|    null|  27|         6| 25000|\n",
      "|     JKL|  22|      null| 40000|\n",
      "|     MNO|  30|         8|  null|\n",
      "+--------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rename column\n",
    "df_pyspark.withColumnRenamed('Name', 'New Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "549f6982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Handing Missing Values\n",
    "spark.read.csv('dataset.csv', header = True, inferSchema=True).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3239bfef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('dataset.csv', header = True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "3f5aa967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+------+\n",
      "| Age|Experience|Salary|\n",
      "+----+----------+------+\n",
      "|  21|         2| 30000|\n",
      "|null|         4| 20000|\n",
      "|  27|         6| 25000|\n",
      "|  22|      null| 40000|\n",
      "|  30|         8|  null|\n",
      "+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Drop columns\n",
    "df_pyspark.drop('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dfc7f3e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "ae82752d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Removes all rows which have null values\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "099c019f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#all - if the entire row is null, any - if any record of the row is null\n",
    "df_pyspark.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "15becd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Threshold - Atleast n non-null values must be there\n",
    "df_pyspark.na.drop(how=\"any\", thresh = 4).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b238a36b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Subset - Delete rows with null values from a particular column\n",
    "df_pyspark.na.drop(how=\"any\",subset=[\"Experience\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2cc592a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF|  0|         4| 20000|\n",
      "|null| 27|         6| 25000|\n",
      "| JKL| 22|         0| 40000|\n",
      "| MNO| 30|         8|     0|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To change Integer columns\n",
    "df_pyspark.na.fill(subset=['Experience', 'Salary', 'Age'], value = 0).show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fb63fca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----+----------+------+\n",
      "|         Name| Age|Experience|Salary|\n",
      "+-------------+----+----------+------+\n",
      "|          ABC|  21|         2| 30000|\n",
      "|          DEF|null|         4| 20000|\n",
      "|No Name given|  27|         6| 25000|\n",
      "|          JKL|  22|      null| 40000|\n",
      "|          MNO|  30|         8|  null|\n",
      "+-------------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(\"No Name given\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "142be1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+\n",
      "|Name| Age|Experience|Salary|\n",
      "+----+----+----------+------+\n",
      "| ABC|  21|         2| 30000|\n",
      "| DEF|null|         4| 20000|\n",
      "|null|  27|         6| 25000|\n",
      "| JKL|  22|      null| 40000|\n",
      "| MNO|  30|         8|  null|\n",
      "+----+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "f6f40cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputer - Used to change the missing values with the mean or any metrics \n",
    "\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "li = ['Age', 'Experience', 'Salary']\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols = li,\n",
    "    outputCols = [\"{}_imputed\".format(c) for c in li]\n",
    ").setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b34ee321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----------+------+-----------+------------------+--------------+\n",
      "|Name| Age|Experience|Salary|Age_imputed|Experience_imputed|Salary_imputed|\n",
      "+----+----+----------+------+-----------+------------------+--------------+\n",
      "| ABC|  21|         2| 30000|         21|                 2|         30000|\n",
      "| DEF|null|         4| 20000|         25|                 4|         20000|\n",
      "|null|  27|         6| 25000|         27|                 6|         25000|\n",
      "| JKL|  22|      null| 40000|         22|                 5|         40000|\n",
      "| MNO|  30|         8|  null|         30|                 8|         28750|\n",
      "+----+----+----------+------+-----------+------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Adds imputed columns to the existing df\n",
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c523663",
   "metadata": {},
   "source": [
    "FILTER OPERATION \n",
    "\n",
    "\n",
    "\n",
    "note - removed null values from the previous dataset used\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "74f434d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Session if not created before\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('dataframe-filter').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "cfbaf00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "| JKL| 22|         1| 40000|\n",
      "| MNO| 30|         8| 60000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('dataset.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "3aef3db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Salary <= 30000\n",
    "df_pyspark.filter(\"Salary<=30000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cae74aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|Name|Age|\n",
      "+----+---+\n",
      "| ABC| 21|\n",
      "| DEF| 24|\n",
      "| GHI| 27|\n",
      "+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter('Salary<=30000').select(['Name','Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "a7fa1165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.filter(df_pyspark['Salary']<=30000).show() #Alternate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "d88f2630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#AND operation\n",
    "df_pyspark.filter((df_pyspark['Salary']<=30000) & \n",
    "                  (df_pyspark['Salary']>=20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6b03e4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| ABC| 21|         2| 30000|\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "| JKL| 22|         1| 40000|\n",
      "| MNO| 30|         8| 60000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#OR operation\n",
    "df_pyspark.filter((df_pyspark['Salary']<=30000) | \n",
    "                  (df_pyspark['Salary']>=20000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "139a87d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+\n",
      "|Name|Age|Experience|Salary|\n",
      "+----+---+----------+------+\n",
      "| DEF| 24|         4| 20000|\n",
      "| GHI| 27|         6| 25000|\n",
      "+----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#NOT operation\n",
    "df_pyspark.filter(~(df_pyspark['Salary']>=30000)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ad9f0",
   "metadata": {},
   "source": [
    "GROUPBY AND AGGREGATE FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f071e449",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = df_pyspark.withColumn('More_Experienced', df_pyspark['Experience']>3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "fcd13266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+----------------+\n",
      "|Name|Age|Experience|Salary|More_Experienced|\n",
      "+----+---+----------+------+----------------+\n",
      "| ABC| 21|         2| 30000|           false|\n",
      "| DEF| 24|         4| 20000|            true|\n",
      "| GHI| 27|         6| 25000|            true|\n",
      "| JKL| 22|         1| 40000|           false|\n",
      "| MNO| 30|         8| 60000|            true|\n",
      "+----+---+----------+------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "c9d3e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- More_Experienced: boolean (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "33e5cca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.group.GroupedData at 0x7f874e9d1970>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "aefc40d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, sum(Age): bigint, sum(Experience): bigint, sum(Salary): bigint]"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "20c78f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------------+-----------+\n",
      "|More_Experienced|sum(Age)|sum(Experience)|sum(Salary)|\n",
      "+----------------+--------+---------------+-----------+\n",
      "|            true|      24|              4|      20000|\n",
      "|           false|      90|              8|     155000|\n",
      "+----------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('More_Experienced').sum().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "08f2511f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------+---------------+-----------+\n",
      "|More_Experienced|avg(Age)|avg(Experience)|avg(Salary)|\n",
      "+----------------+--------+---------------+-----------+\n",
      "|            true|    24.0|            4.0|    20000.0|\n",
      "|           false|    22.5|            2.0|    38750.0|\n",
      "+----------------+--------+---------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('More_Experienced').mean().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "d7de21b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|More_Experienced|count|\n",
      "+----------------+-----+\n",
      "|            true|    1|\n",
      "|           false|    4|\n",
      "+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('More_Experienced').count().show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "5d3c7233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|     175000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "57b10f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|max(Salary)|\n",
      "+-----------+\n",
      "|      60000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.agg({'Salary':'max'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebef60b8",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f7995e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd17da63",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae324cc",
   "metadata": {},
   "source": [
    "MLLIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "8b153515",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = spark.read.csv('dataset.csv', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ac45b8",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a469e9",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521627e3",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "28d9c35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler # Used to join 2 columns to a new feature and consider it an independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4ca71991",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureAssembler = VectorAssembler(inputCols=[\"Age\", \"Experience\"], outputCol= \"AgewithExperience\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "f1d9efb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = featureAssembler.transform(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "3f0d9917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+----------+------+-----------------+\n",
      "|Name|Age|Experience|Salary|AgewithExperience|\n",
      "+----+---+----------+------+-----------------+\n",
      "| ABC| 21|         2| 30000|       [21.0,2.0]|\n",
      "| DEF| 24|         4| 40000|       [24.0,4.0]|\n",
      "| GHI| 27|         6| 50000|       [27.0,6.0]|\n",
      "| JKL| 22|         1| 25000|       [22.0,1.0]|\n",
      "| MNO| 30|         8| 60000|       [30.0,8.0]|\n",
      "+----+---+----------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a646c506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'AgewithExperience']"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d1bf837d",
   "metadata": {},
   "outputs": [],
   "source": [
    "finalized_data = output.select(\"AgewithExperience\",\"Salary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "2c671e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+\n",
      "|AgewithExperience|Salary|\n",
      "+-----------------+------+\n",
      "|       [21.0,2.0]| 30000|\n",
      "|       [24.0,4.0]| 40000|\n",
      "|       [27.0,6.0]| 50000|\n",
      "|       [22.0,1.0]| 25000|\n",
      "|       [30.0,8.0]| 60000|\n",
      "+-----------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "finalized_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9808191",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37597006",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401841b6",
   "metadata": {},
   "source": [
    "Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "ef0d77ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/03/31 13:13:36 WARN Instrumentation: [ea5a7788] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data, test_data = finalized_data.randomSplit([0.75,0.25]) #train_test_split\n",
    "regressor = LinearRegression(featuresCol=\"AgewithExperience\", labelCol='Salary')\n",
    "regressor = regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "5ede53fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0, 5000.0])"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "77202e8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000.00000001955"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "7119cda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_results = regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "1e429c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------+-----------------+\n",
      "|AgewithExperience|Salary|       prediction|\n",
      "+-----------------+------+-----------------+\n",
      "|       [21.0,2.0]| 30000|30000.00000000187|\n",
      "|       [24.0,4.0]| 40000|40000.00000000115|\n",
      "+-----------------+------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_results.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "ad4fd3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.5097612049430609e-09, 2.409094050889239e-18)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_results.meanAbsoluteError,pred_results.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9283f9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
